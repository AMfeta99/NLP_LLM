# Generative AI With Large Language Models
The repository contains 3 folders:
Week 1:
  - Transformers - Basics of Transformers.
  - Prompting and Prompt Engineering - Basics of prompting, prompt engineering and inference configuration parameters (temperature, etc).
  - Pre-training Large Language Models - Basics of how LLMs are pre-trained.
  - Efficient Multi-GPU Compute Strategies - Approaches available for training LLMs in a distributed manner.

Week 2:
  - Instruction Fine-Tuning - Basics of fine-tuning LLMs.
  - Model Evaluation - Metrics and Benchmarks - Details on evaluation metrics and benchmarks for LLMs.
  - Parameter Efficient Fine Tuning (PEFT) - Basics of PEFT, covering LoRA and Soft Prompts.

Week 3:
  - Reinforcement Learning From Human Feedback (RLHF) - Basics of RLHF.
