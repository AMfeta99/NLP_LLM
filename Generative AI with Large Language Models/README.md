# Generative AI With Large Language Models

The repository contains:
## Code Assignments:
 - Summarize_dialogue.ipynb - Dialogue summarization task using generative AI. Perform prompt engineering (compare zero shot, one shot, and few shot inferences).
 - Fine-Tuning Lab.ipynb - Instruction Fine-Tuning using LoRA. Perform PEFT fine-tuning, evaluate the resulting model.
 - RLHF Lab.ipynb - Reinforcement Learning From Human Feedback to align LLMs. Fine-tune FLAN-T5 with RL to generate more-positive summaries.

## Notes & Slides
Week 1:
  - Transformers - Basics of Transformers.
  - Prompting and Prompt Engineering - Basics of prompting, prompt engineering and inference configuration parameters (temperature, etc).
  - Pre-training Large Language Models - Basics of how LLMs are pre-trained.
  - Efficient Multi-GPU Compute Strategies - Approaches available for training LLMs in a distributed manner.

Week 2:
  - Instruction Fine-Tuning - Basics of fine-tuning LLMs.
  - Model Evaluation - Metrics and Benchmarks - Details on evaluation metrics and benchmarks for LLMs.
  - Parameter Efficient Fine Tuning (PEFT) - Basics of PEFT, covering LoRA and Soft Prompts.

Week 3:
  - Reinforcement Learning From Human Feedback (RLHF) - Basics of RLHF.
