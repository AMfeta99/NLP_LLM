# Generative AI With Large Language Models

The repository contains:
## Code Assignments:
 - Summarize_dialogue Lab (Week 1) - Dialogue summarization task using generative AI. Perform prompt engineering (compare zero shot, one shot, and few shot inferences).
 - Fine-Tuning Lab (Week 2) - Instruction Fine-Tuning using LoRA. Perform PEFT fine-tuning, evaluate the resulting model.
 - RLHF Lab (Week 3)- Reinforcement Learning From Human Feedback to align LLMs. Fine-tune FLAN-T5 with RL to generate more-positive summaries.

## Notes & Content
Week 1:
  - Generative AI & LLMs 
  - Transformers 
  - Prompting and Prompt Engineering 
  - Generative AI project lifecycle 
  - Pre-training Large Language Models 
  - Computational challenge in Training LLMs
  - Compute-Optimal Models
  - Domain Adaptation
  - Efficient Multi-GPU Compute Strategies

Week 2:
  - Instruction Fine-Tuning - Basics of fine-tuning LLMs.
  - Model Evaluation - Metrics and Benchmarks - Details on evaluation metrics and benchmarks for LLMs.
  - Parameter Efficient Fine Tuning (PEFT) - Basics of PEFT, covering LoRA and Soft Prompts.

Week 3:
  - Reinforcement Learning From Human Feedback (RLHF) - Basics of RLHF.
