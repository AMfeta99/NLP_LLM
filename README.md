# Natural Language processing & Large Language Models| NLP & LLM
This repository is dedicated to projects and some theoretical material that I used to get into the areas of NLP and LLM in a practical/efficient way.


## Master Thesis
My NLP journey started in 2021-2022 during the development of my master's thesis entitled:
### ["Learning to write medical reports from EEG data"](https://repositorio-aberto.up.pt/handle/10216/144617) 
This aims the development of methods for clinical report writing based on EEG signals, adapting current NLP techniques and state-of-the-art captioning approaches to image, signal and video. 

Due Legal/privacy restrictions, code/implementation cannot be publicly available, however the dissertation has been published and can be accessed at [link](https://repositorio-aberto.up.pt/handle/10216/144617). 

The document includes an introdutions to NLP, state-of-the-art approach for captioning in image/signal/video, details development and comparison of 6 pipelines for EEG captioning.


## Courses
Since then, interest and popularity in the area has been growing, particularly due to the emergence of transformers and LLM applications. So, in an attempt to keep up with this development, I took some courses on more recent topics in the areas offered by some of the big players. This repository includes some theoretical/notes and pratical material of thoses.


### [Finetuning Large Language Models](https://www.coursera.org/programs/bosch-learning-program-8hold/projects/finetuning-large-language-models-project?source=search) (DeepLearning.AI & LAMINI)
  -  Essential finetuning concepts and how to train a large language model.
  -  Understand how finetuning differs from prompt engineering, and when to use both.
  -  Practical experience with real data sets, and how to use techniques for your own projects.

### [Generative AI with Large Language Models](https://www.deeplearning.ai/courses/generative-ai-with-llms/) (DeepLearning.AI & AWS)
  - Deeply understand generative AI, describing the key steps in a typical LLM-based generative AI lifecycle, from data gathering and model selection, to performance evaluation and deployment
  - Describe in detail the transformer architecture that powers LLMs, how theyâ€™re trained, and how fine-tuning enables LLMs to be adapted to a variety of specific use cases
  - Use empirical scaling laws to optimize the model's objective function across dataset size, compute budget, and inference requirements
  - Apply state-of-the art training, tuning, inference, tools, and deployment methods to maximize the performance of models within the specific constraints of your project 
  - Challenges and opportunities that generative AI creates for businesses after hearing stories from industry researchers and practitioners


### [LLMOps](https://www.deeplearning.ai/short-courses/llmops/?utm_campaign=googlecloud3-launch&utm_medium=announcement&utm_source=discourse) (DeepLearning.AI & Google Cloud)

https://github.com/MalayAgr/generative-ai-with-llms-notes?tab=readme-ov-file
